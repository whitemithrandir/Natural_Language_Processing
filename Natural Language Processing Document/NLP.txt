## Introduction

Bu uzmanlık alanındaki önceki kurslarda, bilgisayarla görme sorunlarına yoğun bir şekilde odaklanarak makine öğrenimine ve derin öğrenmeye giriş yaptınız. Sinir ağlarını ve sınıflandırmaları gerçekleştirmek için kalıpları nasıl eşleştirebileceklerini öğrendiniz. Ve sonra onlara nasıl yeni veriler verebilir ve ne görebileceklerini tahmin etmelerini sağlayabilirsiniz. Görüntülerdeki özellikleri tanımlamak ve yalnızca ham piksellerle eşleştirmek yerine bunlara göre sınıflandırmak için kıvrımları kullanarak görüntüler için bunu nasıl biraz daha akıllı hale getireceğinizi öğrendiniz. Bu, görüntüleri çok kontrollü bir ortam kullanmak yerine daha gerçek dünya tarzı olanlara göre sınıflandırmanıza yardımcı oldu.
Bu derste model oluşturmaya geri döneceğiz, ancak metne odaklanacağız ve sınıflandırıcıyı nasıl oluşturabileceğiniz metin modellerine dayanıyor. Metindeki duyarlılığa bakarak başlayacağız ve etiketli metin üzerinde eğitilmiş metni anlayan modellerin nasıl oluşturulacağını öğreneceğiz ve ardından yeni metni gördüklerine göre sınıflandırabiliriz.
Görüntülerle uğraşırken, piksel değerleri zaten sayı olduğu için onları bir sinir ağına beslemek bizim için nispeten kolaydı. Ve ağ, sınıfları etiketlere sığdırmak için kullanılabilecek işlevlerin parametrelerini öğrenebilir. Ama metne ne olur? Bunu cümle ve kelimelerle nasıl yapabiliriz?

## Word based encodings

Bir kümedeki her karakter için karakter kodlamaları alabiliriz. Örneğin, ASCII değerleri. Ama bu bir kelimenin anlamını anlamamıza yardımcı olacak mı? Örneğin, burada gösterildiği gibi 'DİNLE' kelimesini düşünün. Yaygın bir basit karakter kodlaması, burada gösterildiği gibi değerlerle Bilgi Alışverişi için Amerikan Standart Kodu olan ascıı'dir. Yani bu değerler kullanılarak kodlanmış LİSTEN gibi bir kelimeniz olabileceğini düşünebilirsiniz. Ancak bununla ilgili sorun, elbette, kelimenin anlambiliminin harflerle kodlanmamasıdır. Bu, çok farklı ve neredeyse zıt bir anlamı olan, ancak tamamen aynı harflerle 'SESSİZ' kelimesi kullanılarak gösterilebilir. Öyle görünüyor ki, sinir ağını sadece harflerle eğitmek göz korkutucu bir görev olabilir. Peki ya kelimeleri düşünürsek? Ya kelimelere bir değer verebilirsek ve bu değerleri bir ağın eğitiminde kullanabilirsek? Şimdi bir yerlere varabiliriz. Örneğin, bu cümleyi düşünün, Köpeğimi seviyorum. Her kelimeye bir değer vermeye ne dersiniz? Bu değerin ne olduğu önemli değil. Sadece kelime başına bir değerimiz var ve değer her seferinde aynı kelime için aynı. Bu nedenle, cümle için basit bir kodlama, örneğin 'İ' kelimesine bir değer vermek olacaktır. Ardından 'Aşk', 'benim' ve 'köpeğim' kelimelerine sırasıyla 2, 3 ve 4 değerlerini verebiliriz. O zaman köpeğimi seviyorum cümlesi 1, 2, 3, 4 olarak kodlanırdı. Peki ya cümlem varsa, kedimi seviyorsam? 'Kendiminkini seviyorum' kelimelerini zaten 1, 2, 3 olarak kodladık. Böylece bunları yeniden kullanabiliriz ve cat için daha önce görmediğimiz yeni bir simge oluşturabiliriz. Bunu 5 numara yapalım. Şimdi sadece iki kodlama kümesine bakarsak, cümleler arasında bazı benzerlikler görmeye başlayabiliriz. Köpeğimi seviyorum 1, 2, 3, 4 ve kedimi seviyorum 1, 2, 3, 5. Yani bu en azından bir başlangıç ve kelimelere dayalı bir sinir ağını nasıl eğitmeye başlayabiliriz. Neyse ki, TensorFlow ve Keras bize bunu yapmayı çok kolaylaştıran bazı API'ler veriyor. Sonrakilere bakacağız.

## Using APIs

İşte az önce bahsettiğimiz iki cümleyi kodlamak için kod. Sıra sıra açalım. Tensorflow ve keras bize kelimeleri kodlamanın birkaç yolunu sunuyor, ancak odaklanacağım kişi tokenizer. Bu bizim için ağır kaldırmayı halledecek, kelime kodlamaları sözlüğü oluşturacak ve cümlelerden vektörler oluşturacaktır. Cümleleri bir diziye koyacağım. Cümlenin başında olduğu gibi 'Ben'i zaten büyük harfle yazdığımı unutmayın. Daha sonra tokenizer'ın bir örneğini oluştururum. Pasif bir parametre numarası ona hizmet eder. Bu durumda, bu verilerde yalnızca beş farklı kelime olduğu için çok büyük olan 100 kullanıyorum. Çok sayıda metne dayalı bir eğitim seti oluşturuyorsanız, genellikle o metinde kaç tane benzersiz farklı kelime olduğunu bilmezsiniz. Dolayısıyla, bu hiper parametreyi ayarlayarak, belirteçleyicinin yapacağı şey, en üstteki 100 kelimeyi hacme göre almak ve sadece bunları kodlamaktır. Çok sayıda veriyle uğraşırken kullanışlı bir kısayoldur ve bu kursun ilerleyen bölümlerinde gerçek verilerle antrenman yaparken denemeye değer. Bazen daha az kelimenin etkisi minimum ve eğitim doğruluğu olabilir, ancak eğitim süresi çok büyük olabilir, ancak dikkatli kullanın. Belirteçleştiricinin metinlere sığdır yöntemi daha sonra verileri alır ve kodlar. Belirteç oluşturucu, anahtarın sözcük olduğu ve değerin yalnızca yazdırarak inceleyebileceğiniz o sözcüğün belirteci olduğu anahtar değer çiftlerini içeren bir sözlük döndüren bir word ındex özelliği sağlar. Sonuçları burada görebilirsiniz. Büyük harfle yazdığım kelimeyi söylediğimizde, burada küçük harf olduğunu unutmayın. Buraya bir cümle daha ekledim, 'Köpeğimi seviyorsun! ama bunda çok farklı bir şey var. 'Köpek' kelimesinden sonra bir ünlem ekledim! Şimdi, bu sadece köpekten farklı bir kelime olarak mı ele alınmalı? Tabii ki hayır. Bu, tokenizer'ın sizin için yaptığı başka bir şey. Noktalama işaretlerini çıkarır. Bu yüzden daha önce bu yeni veri kümesiyle gördüğümüz kodun sonuçları şöyle görünecek. Anahtar olarak hala sadece 'köpeğimiz' olduğuna dikkat edin. Ünlemin bunu etkilemediğini ve elbette tespit edilen 'sen' kelimesi için yeni bir anahtarımız olduğunu. Bu nedenle, çok basit bir kod yoğun akışı ve keras ile bu metnin kelime tabanlı kodlamalarını oluşturarak metinleri işlemenin başlangıcını gördünüz. Bir sonraki videoda koda bir göz atacağız ve nasıl çalıştığını göreceğiz.

## Notebook for lesson 1

Yani burada derste baktığımız kodu görebilirsiniz. Her şeyden önce, tokenizer sınıfını kullanacaksınız ve bu tokenizer sınıfı tensorflow'da bulunabilir.keras.ön işleme.metin. Tokenizer sınıfı, tokenleri yönetmek, cümleleri token akışlarına dönüştürmek ve tüm bu tür şeyler için bizim için tüm ağır kaldırmayı yapacak. O yüzden başlamak için bir göz atalım. İşte burada sahip olduğum cümle listem var, köpeğimi seviyorum ve kedimi seviyorum, büyük harfle yazıldığımı unutmayın. Ve o zaman tokenizer'ın yapacağı şey, bir örneğini oluşturduğumda, sözlükte maksimum girdi sayısı olarak sahip olmak istediğim bir dizi kelimeyi ileteceğim. Yani bu durumda burada gördüğünüz gibi sadece 5 farklı kelime var, önce köpeğimi sonra kedimi seviyorum. Yani 5'ten büyük olan bu uyuşmuş kelimeler bir nevi gereksiz. Ancak, daha büyük metin kümeleri kullandığınız için, sınıflandırmak istediğiniz binlerce cümleniz varsa, hepsinin içindeki benzersiz kelime sayısını bulmaya çalışmak sizin için zor. Ve böylece yapabileceğiniz şey, bu parametreyi iletmektir ve bana tüm korpustaki en yaygın 100 kelimeyi verin, bana en yaygın 1000 kelimeyi verin ya da her neyse diyebilirsiniz. Bu yüzden, 5'ten fazlasına ihtiyacım olmasa da, burada sadece 100'e temerrüde düşüyorum. Ardından, belirteçleştiricinin metinlere sığdır yöntemini çağırdığımda ve bu listeyi ilettiğimde, yapacağı şey bir takım şeylerdir. Bakacağımız ilk şey, kelime indeksinin anahtar değer çiftlerinin bir listesi olduğu bizim için o kelime indeksini oluşturacağıdır. Burada anahtar kelimedir ve değer o kelimenin simgesidir ve bunu buradan yazdırabiliriz. Yapacağı diğer şeyler, daha sonra bir göz atacağız, ancak örneğin bu cümlelerin her birini bunlar yerine bir belirteç listesine dönüştürmek gibi şeyler. Yani bunu manuel olarak yapmak zorunda değilsiniz ama sadece kelime dizini ile başlayalım. Bu kodu çalıştırdığımda köpeğimi sevdiğimi göreceğiz. Kedi benim sözlüğüm. Bu yüzden tokenize edildi, köpeğimi seviyorum ve sonra kendimde zaten sevgim olduğunu fark ettim, bu yüzden onlarla uğraşmadı ve sonra bana kediler için bir jeton verdi. Cat'i buraya getirdim. Ve videoda da belirtildiği gibi, olaylardan biri, büyük harf I'mi küçük harf I olarak değiştirdiğini fark etmemizdi ve büyük harf ve küçük harf aynı şeyi yapıyor. Örneğin, bunu bir I like this olarak değiştirseydim, ek bir belirteç almazdım, hala I için bir belirtecim var. Bu yüzden büyük / küçük harfe duyarsız hale getiriyor. Buna ek olarak, elbette noktalama işaretlerini ve dilbilgisini kaldırıyor, en azından boşluklar kaldırılmadı. Kelimelere çevrilmiş. Ama örneğin kedimi böyle seviyorum desem yine de tanıdığı 5 jetona sahip olacağım ben veya örneğin yeni bir cümle eklersem sonunda ünlem işareti olan köpeğimi seviyorsunuz. Şimdi, bunu belirttiğimde, alacağım tek şey senin kelimen için yeni bir tane çünkü seni daha önce görmedi. Burada gördüğünüz gibi, bir sen ekledik. Yani bu temel belirteç için bu kadar. Bu, sinir ağlarını eğitmek için metin tabanlı verileri hazırlamanın ilk adımıdır. Bu, daha sonra gömme adı verilen bir şeyle kullanılacaktır. Ve bunu yapmadan önce, cümlelerimizi belirteç tabanlı listelere sokmaya ve bu listeleri aynı boyutta yapmaya bakmak istiyoruz

