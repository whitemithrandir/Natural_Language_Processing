## Introduction
Bu uzmanlık alanındaki önceki kurslarda, bilgisayarla görme sorunlarına yoğun bir şekilde odaklanarak makine öğrenimine ve derin öğrenmeye giriş yaptınız. Sinir ağlarını ve sınıflandırmaları gerçekleştirmek için kalıpları nasıl eşleştirebileceklerini öğrendiniz. Ve sonra onlara nasıl yeni veriler verebilir ve ne görebileceklerini tahmin etmelerini sağlayabilirsiniz. Görüntülerdeki özellikleri tanımlamak ve yalnızca ham piksellerle eşleştirmek yerine bunlara göre sınıflandırmak için kıvrımları kullanarak görüntüler için bunu nasıl biraz daha akıllı hale getireceğinizi öğrendiniz. Bu, görüntüleri çok kontrollü bir ortam kullanmak yerine daha gerçek dünya tarzı olanlara göre sınıflandırmanıza yardımcı oldu.
Bu derste model oluşturmaya geri döneceğiz, ancak metne odaklanacağız ve sınıflandırıcıyı nasıl oluşturabileceğiniz metin modellerine dayanıyor. Metindeki duyarlılığa bakarak başlayacağız ve etiketli metin üzerinde eğitilmiş metni anlayan modellerin nasıl oluşturulacağını öğreneceğiz ve ardından yeni metni gördüklerine göre sınıflandırabiliriz.
Görüntülerle uğraşırken, piksel değerleri zaten sayı olduğu için onları bir sinir ağına beslemek bizim için nispeten kolaydı. Ve ağ, sınıfları etiketlere sığdırmak için kullanılabilecek işlevlerin parametrelerini öğrenebilir. Ama metne ne olur? Bunu cümle ve kelimelerle nasıl yapabiliriz?



## Word based encodings
Bir kümedeki her karakter için karakter kodlamaları alabiliriz. Örneğin, ASCII değerleri. Ama bu bir kelimenin anlamını anlamamıza yardımcı olacak mı? Örneğin, burada gösterildiği gibi 'DİNLE' kelimesini düşünün. Yaygın bir basit karakter kodlaması, burada gösterildiği gibi değerlerle Bilgi Alışverişi için Amerikan Standart Kodu olan ascıı'dir. Yani bu değerler kullanılarak kodlanmış LİSTEN gibi bir kelimeniz olabileceğini düşünebilirsiniz. Ancak bununla ilgili sorun, elbette, kelimenin anlambiliminin harflerle kodlanmamasıdır. Bu, çok farklı ve neredeyse zıt bir anlamı olan, ancak tamamen aynı harflerle 'SESSİZ' kelimesi kullanılarak gösterilebilir. Öyle görünüyor ki, sinir ağını sadece harflerle eğitmek göz korkutucu bir görev olabilir. Peki ya kelimeleri düşünürsek? Ya kelimelere bir değer verebilirsek ve bu değerleri bir ağın eğitiminde kullanabilirsek? Şimdi bir yerlere varabiliriz. Örneğin, bu cümleyi düşünün, Köpeğimi seviyorum. Her kelimeye bir değer vermeye ne dersiniz? Bu değerin ne olduğu önemli değil. Sadece kelime başına bir değerimiz var ve değer her seferinde aynı kelime için aynı. Bu nedenle, cümle için basit bir kodlama, örneğin 'İ' kelimesine bir değer vermek olacaktır. Ardından 'Aşk', 'benim' ve 'köpeğim' kelimelerine sırasıyla 2, 3 ve 4 değerlerini verebiliriz. O zaman köpeğimi seviyorum cümlesi 1, 2, 3, 4 olarak kodlanırdı. Peki ya cümlem varsa, kedimi seviyorsam? 'Kendiminkini seviyorum' kelimelerini zaten 1, 2, 3 olarak kodladık. Böylece bunları yeniden kullanabiliriz ve cat için daha önce görmediğimiz yeni bir simge oluşturabiliriz. Bunu 5 numara yapalım. Şimdi sadece iki kodlama kümesine bakarsak, cümleler arasında bazı benzerlikler görmeye başlayabiliriz. Köpeğimi seviyorum 1, 2, 3, 4 ve kedimi seviyorum 1, 2, 3, 5. Yani bu en azından bir başlangıç ve kelimelere dayalı bir sinir ağını nasıl eğitmeye başlayabiliriz. Neyse ki, TensorFlow ve Keras bize bunu yapmayı çok kolaylaştıran bazı API'ler veriyor. Sonrakilere bakacağız.



## Using APIs
İşte az önce bahsettiğimiz iki cümleyi kodlamak için kod. Sıra sıra açalım. Tensorflow ve keras bize kelimeleri kodlamanın birkaç yolunu sunuyor, ancak odaklanacağım kişi tokenizer. Bu bizim için ağır kaldırmayı halledecek, kelime kodlamaları sözlüğü oluşturacak ve cümlelerden vektörler oluşturacaktır. Cümleleri bir diziye koyacağım. Cümlenin başında olduğu gibi 'Ben'i zaten büyük harfle yazdığımı unutmayın. Daha sonra tokenizer'ın bir örneğini oluştururum. Pasif bir parametre numarası ona hizmet eder. Bu durumda, bu verilerde yalnızca beş farklı kelime olduğu için çok büyük olan 100 kullanıyorum. Çok sayıda metne dayalı bir eğitim seti oluşturuyorsanız, genellikle o metinde kaç tane benzersiz farklı kelime olduğunu bilmezsiniz. Dolayısıyla, bu hiper parametreyi ayarlayarak, belirteçleyicinin yapacağı şey, en üstteki 100 kelimeyi hacme göre almak ve sadece bunları kodlamaktır. Çok sayıda veriyle uğraşırken kullanışlı bir kısayoldur ve bu kursun ilerleyen bölümlerinde gerçek verilerle antrenman yaparken denemeye değer. Bazen daha az kelimenin etkisi minimum ve eğitim doğruluğu olabilir, ancak eğitim süresi çok büyük olabilir, ancak dikkatli kullanın. Belirteçleştiricinin metinlere sığdır yöntemi daha sonra verileri alır ve kodlar. Belirteç oluşturucu, anahtarın sözcük olduğu ve değerin yalnızca yazdırarak inceleyebileceğiniz o sözcüğün belirteci olduğu anahtar değer çiftlerini içeren bir sözlük döndüren bir word ındex özelliği sağlar. Sonuçları burada görebilirsiniz. Büyük harfle yazdığım kelimeyi söylediğimizde, burada küçük harf olduğunu unutmayın. Buraya bir cümle daha ekledim, 'Köpeğimi seviyorsun! ama bunda çok farklı bir şey var. 'Köpek' kelimesinden sonra bir ünlem ekledim! Şimdi, bu sadece köpekten farklı bir kelime olarak mı ele alınmalı? Tabii ki hayır. Bu, tokenizer'ın sizin için yaptığı başka bir şey. Noktalama işaretlerini çıkarır. Bu yüzden daha önce bu yeni veri kümesiyle gördüğümüz kodun sonuçları şöyle görünecek. Anahtar olarak hala sadece 'köpeğimiz' olduğuna dikkat edin. Ünlemin bunu etkilemediğini ve elbette tespit edilen 'sen' kelimesi için yeni bir anahtarımız olduğunu. Bu nedenle, çok basit bir kod yoğun akışı ve keras ile bu metnin kelime tabanlı kodlamalarını oluşturarak metinleri işlemenin başlangıcını gördünüz. Bir sonraki videoda koda bir göz atacağız ve nasıl çalıştığını göreceğiz.



## Notebook for lesson 1
Yani burada derste baktığımız kodu görebilirsiniz. Her şeyden önce, tokenizer sınıfını kullanacaksınız ve bu tokenizer sınıfı tensorflow'da bulunabilir.keras.ön işleme.metin. Tokenizer sınıfı, tokenleri yönetmek, cümleleri token akışlarına dönüştürmek ve tüm bu tür şeyler için bizim için tüm ağır kaldırmayı yapacak. O yüzden başlamak için bir göz atalım. İşte burada sahip olduğum cümle listem var, köpeğimi seviyorum ve kedimi seviyorum, büyük harfle yazıldığımı unutmayın. Ve o zaman tokenizer'ın yapacağı şey, bir örneğini oluşturduğumda, sözlükte maksimum girdi sayısı olarak sahip olmak istediğim bir dizi kelimeyi ileteceğim. Yani bu durumda burada gördüğünüz gibi sadece 5 farklı kelime var, önce köpeğimi sonra kedimi seviyorum. Yani 5'ten büyük olan bu uyuşmuş kelimeler bir nevi gereksiz. Ancak, daha büyük metin kümeleri kullandığınız için, sınıflandırmak istediğiniz binlerce cümleniz varsa, hepsinin içindeki benzersiz kelime sayısını bulmaya çalışmak sizin için zor. Ve böylece yapabileceğiniz şey, bu parametreyi iletmektir ve bana tüm korpustaki en yaygın 100 kelimeyi verin, bana en yaygın 1000 kelimeyi verin ya da her neyse diyebilirsiniz. Bu yüzden, 5'ten fazlasına ihtiyacım olmasa da, burada sadece 100'e temerrüde düşüyorum. Ardından, belirteçleştiricinin metinlere sığdır yöntemini çağırdığımda ve bu listeyi ilettiğimde, yapacağı şey bir takım şeylerdir. Bakacağımız ilk şey, kelime indeksinin anahtar değer çiftlerinin bir listesi olduğu bizim için o kelime indeksini oluşturacağıdır. Burada anahtar kelimedir ve değer o kelimenin simgesidir ve bunu buradan yazdırabiliriz. Yapacağı diğer şeyler, daha sonra bir göz atacağız, ancak örneğin bu cümlelerin her birini bunlar yerine bir belirteç listesine dönüştürmek gibi şeyler. Yani bunu manuel olarak yapmak zorunda değilsiniz ama sadece kelime dizini ile başlayalım. Bu kodu çalıştırdığımda köpeğimi sevdiğimi göreceğiz. Kedi benim sözlüğüm. Bu yüzden tokenize edildi, köpeğimi seviyorum ve sonra kendimde zaten sevgim olduğunu fark ettim, bu yüzden onlarla uğraşmadı ve sonra bana kediler için bir jeton verdi. Cat'i buraya getirdim. Ve videoda da belirtildiği gibi, olaylardan biri, büyük harf I'mi küçük harf I olarak değiştirdiğini fark etmemizdi ve büyük harf ve küçük harf aynı şeyi yapıyor. Örneğin, bunu bir I like this olarak değiştirseydim, ek bir belirteç almazdım, hala I için bir belirtecim var. Bu yüzden büyük / küçük harfe duyarsız hale getiriyor. Buna ek olarak, elbette noktalama işaretlerini ve dilbilgisini kaldırıyor, en azından boşluklar kaldırılmadı. Kelimelere çevrilmiş. Ama örneğin kedimi böyle seviyorum desem yine de tanıdığı 5 jetona sahip olacağım ben veya örneğin yeni bir cümle eklersem sonunda ünlem işareti olan köpeğimi seviyorsunuz. Şimdi, bunu belirttiğimde, alacağım tek şey senin kelimen için yeni bir tane çünkü seni daha önce görmedi. Burada gördüğünüz gibi, bir sen ekledik. Yani bu temel belirteç için bu kadar. Bu, sinir ağlarını eğitmek için metin tabanlı verileri hazırlamanın ilk adımıdır. Bu, daha sonra gömme adı verilen bir şeyle kullanılacaktır. Ve bunu yapmadan önce, cümlelerimizi belirteç tabanlı listelere sokmaya ve bu listeleri aynı boyutta yapmaya bakmak istiyoruz



## Text to sequence
Önceki videoda, kelimeleri ve cümleleri nasıl belirteceğinizi gördünüz, bir korpus yapmak için tüm kelimelerin bir sözlüğünü oluşturdunuz. Bir sonraki adım, cümlelerinizi bu belirteçlere dayalı değer listelerine dönüştürmek olacaktır. Onlara sahip olduğunuzda, muhtemelen her cümleyi aynı uzunlukta yapmak için değil, bu listeleri de değiştirmeniz gerekecektir, aksi takdirde onlarla bir sinir ağı kurmak zor olabilir. Görüntüleri yaparken, sinir ağına beslediğimiz görüntünün boyutuna sahip bir giriş katmanı tanımladığımızı hatırlayın. Görüntülerin farklı boyutlarda olduğu durumlarda, bunları sığacak şekilde yeniden boyutlandıracağız. Aynı şeyi metinle de yüzleşeceksin. Neyse ki, TensorFlow bu sorunları ele almak için API'ler içerir. Bu videoda bunlara bakacağız. Bir dizi listesi oluşturmakla başlayalım, oluşturduğumuz belirteçlerle kodlanmış cümleler ve üzerinde çalıştığımız kodu buna güncelledim. Öncelikle cümle listesinin sonuna bir cümle daha ekledim. Önceki cümlelerin hepsinde dört kelime olduğunu unutmayın. Yani bu biraz daha uzun. Bunu birazdan dolguyu göstermek için kullanacağız. Bir sonraki kod parçası, metinleri dizilere almak için tokenizer'ı çağırdığım ve bunları benim için bir dizi diziye dönüştürecek olan koddur. Yani bu kodu çalıştırırsam, çıktı bu olacaktır. En üstte yeni sözlük var. Amazing, think, is ve do gibi yeni kelimelerim için yeni belirteçlerle. En altta, tamsayı listelerine kodlanmış cümleler listem var ve sözcüklerin yerini belirteçler alıyor. Örneğin, köpeğimi seviyorum 4, 2, 1, 3 olur. Bununla ilgili daha sonra kullanacağınız gerçekten kullanışlı bir şey, çağrılan metin dizilerinin herhangi bir cümle kümesini alabilmesidir, bu nedenle bunları aktarılandan öğrendiği kelime kümesine göre kodlayabilir. metinlere sığdır. Biraz ileriyi düşünürseniz bu çok önemlidir. Bir metin dizisi üzerinde bir sinir ağı eğitirseniz ve metnin ondan oluşturulmuş bir kelime dizini varsa, o zaman tren modeliyle çıkarım yapmak istediğinizde, çıkarmak istediğiniz metni aynı kelime dizini ile kodlamanız gerekir, aksi takdirde anlamsız olur. Yani bu kodu düşünürseniz, sonucun ne olmasını bekliyorsunuz? Burada aşk, benim ve köpek gibi tanıdık kelimeler var ama aynı zamanda daha önce görünmeyenler de var. Bu kodu çalıştırırsam, alacağım şey bu olur. Kolaylık olması için altına sözlüğü ekledim. Bu yüzden köpeğimin hala 4, 2, 1, 3 olarak kodlanmasını çok seviyorum, bu da 'Köpeğimi seviyorum' ve kelime Kelime Dizininde olmadığı için 'gerçekten' kayboluyor ve 'köpeğim denizayımı seviyor' 1, 3, 1 olarak kodlanacaktı, bu sadece 'köpeğim benim'.



## Looking more at the Tokenizer
Peki bundan ne öğreneceğiz? Her şeyden önce, geniş bir kelime hazinesi elde etmek için gerçekten çok fazla eğitim verisine ihtiyacımız var ya da az önce yaptığımız gibi köpeğim benim gibi cümlelerle sonuçlanabiliriz. İkincisi, çoğu durumda, görünmeyen kelimeleri görmezden gelmek yerine, görünmeyen bir kelimeyle karşılaşıldığında özel bir değer koymak iyi bir fikirdir. Bunu tokenizer'daki bir özellik ile yapabilirsiniz. Bir bakalım. İşte hem orijinal cümleleri hem de test verilerini gösteren tam kod. Değiştirdiğim şey, tokenizer yapıcısına bir özellik oov belirteci eklemektir. Artık, dış sözcük dağarcığı için oov belirtecinin sözcük dizininde olmayan sözcükler için kullanılmasını istediğimi belirttiğimi görebilirsiniz. Burada istediğiniz her şeyi kullanabilirsiniz, ancak bunun gerçek bir kelimeyle karıştırılmayan benzersiz ve farklı bir şey olması gerektiğini unutmayın. Şimdi, eğer bu kodu çalıştırırsam, test dizilerimi şuna benzeteceğim. Dizin kelimesini altına yapıştırdım, böylece arayabilirsin. İlk cümle, kelime dağarcığım bitti, köpeğimi seviyorum olacak. İkincisi, köpeğim oov olacak, oov'um Hala sözdizimsel olarak harika değil, ama daha iyisini yapıyor. Korpus büyüdükçe ve dizinde daha fazla kelime bulundukça, umarım daha önce görülmemiş cümleler daha iyi kapsama sahip olur. Sırada dolgu var. Resimleri işlemek için sinir ağları kurarken daha önce de belirttiğimiz gibi. Onları eğitim için ağa beslediğimizde, boyut olarak tek tip olmalarına ihtiyacımız vardı. Genellikle, görüntüyü örneğin sığacak şekilde yeniden boyutlandırmak için oluşturucuları kullanırız. Metinlerle antrenman yapmadan önce benzer bir gereksinimle karşılaşacaksınız, bir miktar tekdüzelik boyutuna sahip olmamız gerekiyordu, bu yüzden dolgu oradaki arkadaşınız.

## Padding
Bu yüzden dolguyu işlemek için kodda birkaç değişiklik yaptım. İşte tam liste ve parça parça parça edeceğiz. İlk olarak, padding işlevlerini kullanmak için pad dizilerini tensorflow'dan içe aktarmanız gerekir.keras.ön işleme.Sıra. Ardından, belirteç oluşturucu dizileri oluşturduktan sonra, bu diziler, bu şekilde doldurulmaları için altlık dizilerine geçirilebilir. Sonuç oldukça yalındır. Artık cümle listesinin bir matrise doldurulduğunu ve matristeki her satırın aynı uzunluğa sahip olduğunu görebilirsiniz. Bunu, cümlenin önüne uygun sayıda sıfır koyarak başardı. Yani 5-3-2-4 cümlesi söz konusu olduğunda, aslında hiçbir şey yapmadı. Buradaki daha uzun cümle durumunda, herhangi bir şey yapmasına gerek yoktu. Çoğu zaman, dolgunun cümleden sonra olduğu ve daha önce gördüğünüz gibi olmadığı örnekleri görürsünüz. Benim gibi, bu konuda daha rahatsanız, kodu buna değiştirebilir ve parametre dolgusunu ekleyerek post'a eşittir. Matris genişliğinin en uzun cümle ile aynı olduğunu fark etmiş olabilirsiniz. Ancak bunu maxlen parametresiyle geçersiz kılabilirsiniz. Örneğin, yalnızca cümlelerinizin en fazla beş kelimeye sahip olmasını istiyorsanız. Maxlen'in beşe eşit olduğunu söyleyebilirsin. Bu elbette soruya yol açacaktır. Maksimum uzunluktan daha uzun cümlelerim varsa, o zaman bilgiyi kaybederim ama nereden. Dolguda olduğu gibi varsayılan değer pre'dir, bu da cümlenin başından itibaren kaybedeceğiniz anlamına gelir. Bunun yerine sondan kaybetmek için bunu geçersiz kılmak istiyorsanız, bunu aşağıdaki gibi kesme parametresiyle yapabilirsiniz. Artık cümlelerinizi nasıl kodlayacağınızı, bunları nasıl not alacağınızı ve daha önce görülmemiş cümleleri kelime dışı karakterler kullanarak kodlamak için Kelime indekslemeyi nasıl kullanacağınızı gördünüz. Ama bunu çok basit kodlanmış verilerle yaptınız. Bir ekran yayınındaki kodlanmış eyleme bir göz atalım ve sonra geri dönüp çok daha karmaşık verilerin nasıl kullanılacağına bakalım.