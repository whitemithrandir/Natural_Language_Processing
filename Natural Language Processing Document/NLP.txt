## Introduction
Bu uzmanlık alanındaki önceki kurslarda, bilgisayarla görme sorunlarına yoğun bir şekilde odaklanarak makine öğrenimine ve derin öğrenmeye giriş yaptınız. Sinir ağlarını ve sınıflandırmaları gerçekleştirmek için kalıpları nasıl eşleştirebileceklerini öğrendiniz. Ve sonra onlara nasıl yeni veriler verebilir ve ne görebileceklerini tahmin etmelerini sağlayabilirsiniz. Görüntülerdeki özellikleri tanımlamak ve yalnızca ham piksellerle eşleştirmek yerine bunlara göre sınıflandırmak için kıvrımları kullanarak görüntüler için bunu nasıl biraz daha akıllı hale getireceğinizi öğrendiniz. Bu, görüntüleri çok kontrollü bir ortam kullanmak yerine daha gerçek dünya tarzı olanlara göre sınıflandırmanıza yardımcı oldu.
Bu derste model oluşturmaya geri döneceğiz, ancak metne odaklanacağız ve sınıflandırıcıyı nasıl oluşturabileceğiniz metin modellerine dayanıyor. Metindeki duyarlılığa bakarak başlayacağız ve etiketli metin üzerinde eğitilmiş metni anlayan modellerin nasıl oluşturulacağını öğreneceğiz ve ardından yeni metni gördüklerine göre sınıflandırabiliriz.
Görüntülerle uğraşırken, piksel değerleri zaten sayı olduğu için onları bir sinir ağına beslemek bizim için nispeten kolaydı. Ve ağ, sınıfları etiketlere sığdırmak için kullanılabilecek işlevlerin parametrelerini öğrenebilir. Ama metne ne olur? Bunu cümle ve kelimelerle nasıl yapabiliriz?



## Word based encodings
Bir kümedeki her karakter için karakter kodlamaları alabiliriz. Örneğin, ASCII değerleri. Ama bu bir kelimenin anlamını anlamamıza yardımcı olacak mı? Örneğin, burada gösterildiği gibi 'DİNLE' kelimesini düşünün. Yaygın bir basit karakter kodlaması, burada gösterildiği gibi değerlerle Bilgi Alışverişi için Amerikan Standart Kodu olan ascıı'dir. Yani bu değerler kullanılarak kodlanmış LİSTEN gibi bir kelimeniz olabileceğini düşünebilirsiniz. Ancak bununla ilgili sorun, elbette, kelimenin anlambiliminin harflerle kodlanmamasıdır. Bu, çok farklı ve neredeyse zıt bir anlamı olan, ancak tamamen aynı harflerle 'SESSİZ' kelimesi kullanılarak gösterilebilir. Öyle görünüyor ki, sinir ağını sadece harflerle eğitmek göz korkutucu bir görev olabilir. Peki ya kelimeleri düşünürsek? Ya kelimelere bir değer verebilirsek ve bu değerleri bir ağın eğitiminde kullanabilirsek? Şimdi bir yerlere varabiliriz. Örneğin, bu cümleyi düşünün, Köpeğimi seviyorum. Her kelimeye bir değer vermeye ne dersiniz? Bu değerin ne olduğu önemli değil. Sadece kelime başına bir değerimiz var ve değer her seferinde aynı kelime için aynı. Bu nedenle, cümle için basit bir kodlama, örneğin 'İ' kelimesine bir değer vermek olacaktır. Ardından 'Aşk', 'benim' ve 'köpeğim' kelimelerine sırasıyla 2, 3 ve 4 değerlerini verebiliriz. O zaman köpeğimi seviyorum cümlesi 1, 2, 3, 4 olarak kodlanırdı. Peki ya cümlem varsa, kedimi seviyorsam? 'Kendiminkini seviyorum' kelimelerini zaten 1, 2, 3 olarak kodladık. Böylece bunları yeniden kullanabiliriz ve cat için daha önce görmediğimiz yeni bir simge oluşturabiliriz. Bunu 5 numara yapalım. Şimdi sadece iki kodlama kümesine bakarsak, cümleler arasında bazı benzerlikler görmeye başlayabiliriz. Köpeğimi seviyorum 1, 2, 3, 4 ve kedimi seviyorum 1, 2, 3, 5. Yani bu en azından bir başlangıç ve kelimelere dayalı bir sinir ağını nasıl eğitmeye başlayabiliriz. Neyse ki, TensorFlow ve Keras bize bunu yapmayı çok kolaylaştıran bazı API'ler veriyor. Sonrakilere bakacağız.



## Using APIs
İşte az önce bahsettiğimiz iki cümleyi kodlamak için kod. Sıra sıra açalım. Tensorflow ve keras bize kelimeleri kodlamanın birkaç yolunu sunuyor, ancak odaklanacağım kişi tokenizer. Bu bizim için ağır kaldırmayı halledecek, kelime kodlamaları sözlüğü oluşturacak ve cümlelerden vektörler oluşturacaktır. Cümleleri bir diziye koyacağım. Cümlenin başında olduğu gibi 'Ben'i zaten büyük harfle yazdığımı unutmayın. Daha sonra tokenizer'ın bir örneğini oluştururum. Pasif bir parametre numarası ona hizmet eder. Bu durumda, bu verilerde yalnızca beş farklı kelime olduğu için çok büyük olan 100 kullanıyorum. Çok sayıda metne dayalı bir eğitim seti oluşturuyorsanız, genellikle o metinde kaç tane benzersiz farklı kelime olduğunu bilmezsiniz. Dolayısıyla, bu hiper parametreyi ayarlayarak, belirteçleyicinin yapacağı şey, en üstteki 100 kelimeyi hacme göre almak ve sadece bunları kodlamaktır. Çok sayıda veriyle uğraşırken kullanışlı bir kısayoldur ve bu kursun ilerleyen bölümlerinde gerçek verilerle antrenman yaparken denemeye değer. Bazen daha az kelimenin etkisi minimum ve eğitim doğruluğu olabilir, ancak eğitim süresi çok büyük olabilir, ancak dikkatli kullanın. Belirteçleştiricinin metinlere sığdır yöntemi daha sonra verileri alır ve kodlar. Belirteç oluşturucu, anahtarın sözcük olduğu ve değerin yalnızca yazdırarak inceleyebileceğiniz o sözcüğün belirteci olduğu anahtar değer çiftlerini içeren bir sözlük döndüren bir word ındex özelliği sağlar. Sonuçları burada görebilirsiniz. Büyük harfle yazdığım kelimeyi söylediğimizde, burada küçük harf olduğunu unutmayın. Buraya bir cümle daha ekledim, 'Köpeğimi seviyorsun! ama bunda çok farklı bir şey var. 'Köpek' kelimesinden sonra bir ünlem ekledim! Şimdi, bu sadece köpekten farklı bir kelime olarak mı ele alınmalı? Tabii ki hayır. Bu, tokenizer'ın sizin için yaptığı başka bir şey. Noktalama işaretlerini çıkarır. Bu yüzden daha önce bu yeni veri kümesiyle gördüğümüz kodun sonuçları şöyle görünecek. Anahtar olarak hala sadece 'köpeğimiz' olduğuna dikkat edin. Ünlemin bunu etkilemediğini ve elbette tespit edilen 'sen' kelimesi için yeni bir anahtarımız olduğunu. Bu nedenle, çok basit bir kod yoğun akışı ve keras ile bu metnin kelime tabanlı kodlamalarını oluşturarak metinleri işlemenin başlangıcını gördünüz. Bir sonraki videoda koda bir göz atacağız ve nasıl çalıştığını göreceğiz.



## Notebook for lesson 1
Yani burada derste baktığımız kodu görebilirsiniz. Her şeyden önce, tokenizer sınıfını kullanacaksınız ve bu tokenizer sınıfı tensorflow'da bulunabilir.keras.ön işleme.metin. Tokenizer sınıfı, tokenleri yönetmek, cümleleri token akışlarına dönüştürmek ve tüm bu tür şeyler için bizim için tüm ağır kaldırmayı yapacak. O yüzden başlamak için bir göz atalım. İşte burada sahip olduğum cümle listem var, köpeğimi seviyorum ve kedimi seviyorum, büyük harfle yazıldığımı unutmayın. Ve o zaman tokenizer'ın yapacağı şey, bir örneğini oluşturduğumda, sözlükte maksimum girdi sayısı olarak sahip olmak istediğim bir dizi kelimeyi ileteceğim. Yani bu durumda burada gördüğünüz gibi sadece 5 farklı kelime var, önce köpeğimi sonra kedimi seviyorum. Yani 5'ten büyük olan bu uyuşmuş kelimeler bir nevi gereksiz. Ancak, daha büyük metin kümeleri kullandığınız için, sınıflandırmak istediğiniz binlerce cümleniz varsa, hepsinin içindeki benzersiz kelime sayısını bulmaya çalışmak sizin için zor. Ve böylece yapabileceğiniz şey, bu parametreyi iletmektir ve bana tüm korpustaki en yaygın 100 kelimeyi verin, bana en yaygın 1000 kelimeyi verin ya da her neyse diyebilirsiniz. Bu yüzden, 5'ten fazlasına ihtiyacım olmasa da, burada sadece 100'e temerrüde düşüyorum. Ardından, belirteçleştiricinin metinlere sığdır yöntemini çağırdığımda ve bu listeyi ilettiğimde, yapacağı şey bir takım şeylerdir. Bakacağımız ilk şey, kelime indeksinin anahtar değer çiftlerinin bir listesi olduğu bizim için o kelime indeksini oluşturacağıdır. Burada anahtar kelimedir ve değer o kelimenin simgesidir ve bunu buradan yazdırabiliriz. Yapacağı diğer şeyler, daha sonra bir göz atacağız, ancak örneğin bu cümlelerin her birini bunlar yerine bir belirteç listesine dönüştürmek gibi şeyler. Yani bunu manuel olarak yapmak zorunda değilsiniz ama sadece kelime dizini ile başlayalım. Bu kodu çalıştırdığımda köpeğimi sevdiğimi göreceğiz. Kedi benim sözlüğüm. Bu yüzden tokenize edildi, köpeğimi seviyorum ve sonra kendimde zaten sevgim olduğunu fark ettim, bu yüzden onlarla uğraşmadı ve sonra bana kediler için bir jeton verdi. Cat'i buraya getirdim. Ve videoda da belirtildiği gibi, olaylardan biri, büyük harf I'mi küçük harf I olarak değiştirdiğini fark etmemizdi ve büyük harf ve küçük harf aynı şeyi yapıyor. Örneğin, bunu bir I like this olarak değiştirseydim, ek bir belirteç almazdım, hala I için bir belirtecim var. Bu yüzden büyük / küçük harfe duyarsız hale getiriyor. Buna ek olarak, elbette noktalama işaretlerini ve dilbilgisini kaldırıyor, en azından boşluklar kaldırılmadı. Kelimelere çevrilmiş. Ama örneğin kedimi böyle seviyorum desem yine de tanıdığı 5 jetona sahip olacağım ben veya örneğin yeni bir cümle eklersem sonunda ünlem işareti olan köpeğimi seviyorsunuz. Şimdi, bunu belirttiğimde, alacağım tek şey senin kelimen için yeni bir tane çünkü seni daha önce görmedi. Burada gördüğünüz gibi, bir sen ekledik. Yani bu temel belirteç için bu kadar. Bu, sinir ağlarını eğitmek için metin tabanlı verileri hazırlamanın ilk adımıdır. Bu, daha sonra gömme adı verilen bir şeyle kullanılacaktır. Ve bunu yapmadan önce, cümlelerimizi belirteç tabanlı listelere sokmaya ve bu listeleri aynı boyutta yapmaya bakmak istiyoruz



## Text to sequence
Önceki videoda, kelimeleri ve cümleleri nasıl belirteceğinizi gördünüz, bir korpus yapmak için tüm kelimelerin bir sözlüğünü oluşturdunuz. Bir sonraki adım, cümlelerinizi bu belirteçlere dayalı değer listelerine dönüştürmek olacaktır. Onlara sahip olduğunuzda, muhtemelen her cümleyi aynı uzunlukta yapmak için değil, bu listeleri de değiştirmeniz gerekecektir, aksi takdirde onlarla bir sinir ağı kurmak zor olabilir. Görüntüleri yaparken, sinir ağına beslediğimiz görüntünün boyutuna sahip bir giriş katmanı tanımladığımızı hatırlayın. Görüntülerin farklı boyutlarda olduğu durumlarda, bunları sığacak şekilde yeniden boyutlandıracağız. Aynı şeyi metinle de yüzleşeceksin. Neyse ki, TensorFlow bu sorunları ele almak için API'ler içerir. Bu videoda bunlara bakacağız. Bir dizi listesi oluşturmakla başlayalım, oluşturduğumuz belirteçlerle kodlanmış cümleler ve üzerinde çalıştığımız kodu buna güncelledim. Öncelikle cümle listesinin sonuna bir cümle daha ekledim. Önceki cümlelerin hepsinde dört kelime olduğunu unutmayın. Yani bu biraz daha uzun. Bunu birazdan dolguyu göstermek için kullanacağız. Bir sonraki kod parçası, metinleri dizilere almak için tokenizer'ı çağırdığım ve bunları benim için bir dizi diziye dönüştürecek olan koddur. Yani bu kodu çalıştırırsam, çıktı bu olacaktır. En üstte yeni sözlük var. Amazing, think, is ve do gibi yeni kelimelerim için yeni belirteçlerle. En altta, tamsayı listelerine kodlanmış cümleler listem var ve sözcüklerin yerini belirteçler alıyor. Örneğin, köpeğimi seviyorum 4, 2, 1, 3 olur. Bununla ilgili daha sonra kullanacağınız gerçekten kullanışlı bir şey, çağrılan metin dizilerinin herhangi bir cümle kümesini alabilmesidir, bu nedenle bunları aktarılandan öğrendiği kelime kümesine göre kodlayabilir. metinlere sığdır. Biraz ileriyi düşünürseniz bu çok önemlidir. Bir metin dizisi üzerinde bir sinir ağı eğitirseniz ve metnin ondan oluşturulmuş bir kelime dizini varsa, o zaman tren modeliyle çıkarım yapmak istediğinizde, çıkarmak istediğiniz metni aynı kelime dizini ile kodlamanız gerekir, aksi takdirde anlamsız olur. Yani bu kodu düşünürseniz, sonucun ne olmasını bekliyorsunuz? Burada aşk, benim ve köpek gibi tanıdık kelimeler var ama aynı zamanda daha önce görünmeyenler de var. Bu kodu çalıştırırsam, alacağım şey bu olur. Kolaylık olması için altına sözlüğü ekledim. Bu yüzden köpeğimin hala 4, 2, 1, 3 olarak kodlanmasını çok seviyorum, bu da 'Köpeğimi seviyorum' ve kelime Kelime Dizininde olmadığı için 'gerçekten' kayboluyor ve 'köpeğim denizayımı seviyor' 1, 3, 1 olarak kodlanacaktı, bu sadece 'köpeğim benim'.



## Looking more at the Tokenizer
Peki bundan ne öğreneceğiz? Her şeyden önce, geniş bir kelime hazinesi elde etmek için gerçekten çok fazla eğitim verisine ihtiyacımız var ya da az önce yaptığımız gibi köpeğim benim gibi cümlelerle sonuçlanabiliriz. İkincisi, çoğu durumda, görünmeyen kelimeleri görmezden gelmek yerine, görünmeyen bir kelimeyle karşılaşıldığında özel bir değer koymak iyi bir fikirdir. Bunu tokenizer'daki bir özellik ile yapabilirsiniz. Bir bakalım. İşte hem orijinal cümleleri hem de test verilerini gösteren tam kod. Değiştirdiğim şey, tokenizer yapıcısına bir özellik oov belirteci eklemektir. Artık, dış sözcük dağarcığı için oov belirtecinin sözcük dizininde olmayan sözcükler için kullanılmasını istediğimi belirttiğimi görebilirsiniz. Burada istediğiniz her şeyi kullanabilirsiniz, ancak bunun gerçek bir kelimeyle karıştırılmayan benzersiz ve farklı bir şey olması gerektiğini unutmayın. Şimdi, eğer bu kodu çalıştırırsam, test dizilerimi şuna benzeteceğim. Dizin kelimesini altına yapıştırdım, böylece arayabilirsin. İlk cümle, kelime dağarcığım bitti, köpeğimi seviyorum olacak. İkincisi, köpeğim oov olacak, oov'um Hala sözdizimsel olarak harika değil, ama daha iyisini yapıyor. Korpus büyüdükçe ve dizinde daha fazla kelime bulundukça, umarım daha önce görülmemiş cümleler daha iyi kapsama sahip olur. Sırada dolgu var. Resimleri işlemek için sinir ağları kurarken daha önce de belirttiğimiz gibi. Onları eğitim için ağa beslediğimizde, boyut olarak tek tip olmalarına ihtiyacımız vardı. Genellikle, görüntüyü örneğin sığacak şekilde yeniden boyutlandırmak için oluşturucuları kullanırız. Metinlerle antrenman yapmadan önce benzer bir gereksinimle karşılaşacaksınız, bir miktar tekdüzelik boyutuna sahip olmamız gerekiyordu, bu yüzden dolgu oradaki arkadaşınız.

## Padding
Bu yüzden dolguyu işlemek için kodda birkaç değişiklik yaptım. İşte tam liste ve parça parça parça edeceğiz. İlk olarak, padding işlevlerini kullanmak için pad dizilerini tensorflow'dan içe aktarmanız gerekir.keras.ön işleme.Sıra. Ardından, belirteç oluşturucu dizileri oluşturduktan sonra, bu diziler, bu şekilde doldurulmaları için altlık dizilerine geçirilebilir. Sonuç oldukça yalındır. Artık cümle listesinin bir matrise doldurulduğunu ve matristeki her satırın aynı uzunluğa sahip olduğunu görebilirsiniz. Bunu, cümlenin önüne uygun sayıda sıfır koyarak başardı. Yani 5-3-2-4 cümlesi söz konusu olduğunda, aslında hiçbir şey yapmadı. Buradaki daha uzun cümle durumunda, herhangi bir şey yapmasına gerek yoktu. Çoğu zaman, dolgunun cümleden sonra olduğu ve daha önce gördüğünüz gibi olmadığı örnekleri görürsünüz. Benim gibi, bu konuda daha rahatsanız, kodu buna değiştirebilir ve parametre dolgusunu ekleyerek post'a eşittir. Matris genişliğinin en uzun cümle ile aynı olduğunu fark etmiş olabilirsiniz. Ancak bunu maxlen parametresiyle geçersiz kılabilirsiniz. Örneğin, yalnızca cümlelerinizin en fazla beş kelimeye sahip olmasını istiyorsanız. Maxlen'in beşe eşit olduğunu söyleyebilirsin. Bu elbette soruya yol açacaktır. Maksimum uzunluktan daha uzun cümlelerim varsa, o zaman bilgiyi kaybederim ama nereden. Dolguda olduğu gibi varsayılan değer pre'dir, bu da cümlenin başından itibaren kaybedeceğiniz anlamına gelir. Bunun yerine sondan kaybetmek için bunu geçersiz kılmak istiyorsanız, bunu aşağıdaki gibi kesme parametresiyle yapabilirsiniz. Artık cümlelerinizi nasıl kodlayacağınızı, bunları nasıl not alacağınızı ve daha önce görülmemiş cümleleri kelime dışı karakterler kullanarak kodlamak için Kelime indekslemeyi nasıl kullanacağınızı gördünüz. Ama bunu çok basit kodlanmış verilerle yaptınız. Bir ekran yayınındaki kodlanmış eyleme bir göz atalım ve sonra geri dönüp çok daha karmaşık verilerin nasıl kullanılacağına bakalım.


## Padding Notebook for lesson 2
Yaptığımız bazı temel metin ön işleme işlemlerine bir göz atalım. Her şeyden önce, Tensorflow'da birkaç API kullanacağız. Tokenizer ve pad dizileri var. Tokenizer tensorflow'da bulunabilir.keras.ön işleme.metin ve bloknot dizileri tensorflow'da bulunabilir.keras.ön işleme.sıra. Tokenize edeceğimiz ilk çalışma grubumuz burada listelenmiştir. Bu benim cümle listem. Köpeğimi seviyorum, kedimi seviyorum. Köpeğimi seviyorsun ve sence köpeğim harika mı? Belirteç oluşturucuda yeni bir belirteç oluşturacağım. Korpusundaki maksimum kelime sayısının 100 olması gerektiğini söyleyeceğim. Açıkçası burada 100'den az kelime veya burada 100 farklı, benzersiz ve farklı kelime var. Ayrıca bunları kelime dağarcığından belirteceğim belirteç şöyle görünmelidir. Sadece OOV olduğunu söyleyeceğim. İstediğim her şeyi buraya koyabilirim. Ama her zaman metnin gövdesinde görmeyi beklemeyeceğiniz bir şey olmalı, bu yüzden köşeli parantezler ve oov'ları bu şekilde koydum. Sonra, tokenizer'ı söylediğimde.fitontexts cümleleri, o zaman yapacağı şey, burada bulunan benzersiz kelimeler için benzersiz bir anahtar değerler kümesi oluşturarak bir belirteç oluşturacağıdır. Anahtar bir aşk olacağım ya da anahtar olacağım, bu tür şeyler. Anahtar değer çiftlerinin olduğu, bir kelimenin anahtar olduğu ve bu anahtarın bir değere sahip olduğu bir kelime dizini oluşturacaktır. Örneğin, aşk bir anahtar olur, ben bir anahtar olurum, köpek bir anahtar olur. Bu tür şeyler. Bu kelime dizini, daha sonra sadece kelime dizini özelliğini çağırarak tokenizer'dan çekebilirim. Bakabileceğimiz bir word dizin değişkeni bulabilirim. Bir sonraki şey, dizileri kullanabilmek için tokenizer üzerindedir. Daha sonra sahip olduğu metin cümlelerini dizilere dönüştürecek ve cümledeki her kelimeyi o kelimenin anahtarının değeriyle değiştirecektir. Örneğin, eğer kelime 2 anahtarına sahip olabilirsem ve aşk 3 anahtarına sahipse. Sevdiğimi söylemek yerine, bir dizide 2, 3 olurdu, bu tür şeyler. Bu bana dizileri verecek. Ardından, tüm bu dizileri aynı boyutta veya aynı genişlikte yapmak için ped dizilerini kullanacağım. Bu cümlelerin bazılarında dört kelime olduğunu görüyorsunuz. Bazılarında daha fazlası var, sanırım bunda yedi kelime var. Bir sinir ağını eğitirken, tüm verilerin aynı boyutta olmasını isteriz. Konvolüsyonel sinir ağları ile ne zaman yaptığımızı hatırlarsanız, bazı durumlarda, Moda-MNİST'TE olduğu gibi, veriler her zaman 28'e 28 idi. Ya da diğer durumlarda, kediler, köpekler ya da insanlara karşı atlarda olduğu gibi, verileri yüklerken yeniden boyutlandırdık. Ped dizileri bunun için arkadaşımız. Bu durumda, ped dizilerini aldığımda, yeni oluşturduğumuz dizileri geçeceğim. Post'a eşit dolgu ve kesme post'a eşit olmak üzere birkaç parametre belirteceğim. Buradaki fikir, eğer cümlem dizinin genişliğinden daha kısaysa, cümlemi yazacak olmasıdır. Altlık dizileriyle ilgili varsayılan deneyim, dolgunun tümceden önce olacağıdır. Örneğin, bu durumda cümlede yedi kelimemiz var. Bu durumda dört tane var. Bu cümlelerin üçü de, cümledeki gerçek kelimelerin belirteçlerinden önce üç sıfırla doldurulur. Benim için 0,0,0 jeton, aşk için jeton, benim için jeton, bu tür şeyler olacak. Padding equals post belirterek, sıfırların cümleden sonra geldiği yerde tam tersini yaparız. Şimdi bunu nasıl yapacağın gerçekten sana bağlı. Cümlenin ardından olmasını tercih ederim, böylece cümleyi baştan okuyabilirim çünkü soldan sağa okuyorum. Ama bu size ve senaryonuza bağlı. Sence en iyisi ne? Varsayılan değer pre'dir. Post belirterek buradaki varsayılanı geçersiz kılıyorum. Benzer şekilde, kullanacağımız dizinin genişliğini belirlemeye karar verirsek. Örneğin, bu durumda yedi kelime vardır ve bunun gibi maxlen beşe eşittir diyerek sadece beş kelime olmasını isteriz. O zaman ne olacak, cümle beşe bölünecek. Varsayılan davranış, kesme eşittir belirterek baştan kesmektir post , sondan kesecektir. Bu durumda, eğer maxlen beş ise, ilk beş kelimeyi alırız; Sence köpeğim ve son iki kelimeyi kaybederiz. Eşit gönderileri kesmeseydik, varsayılan olarak pre olur ya da bunu pre olarak böyle ayarlarsak, köpeğimin harika olduğunu düşünen son beş kelimeyi alır, bu tür bir şey. Bunu tekrar yazıya çevireceğim. Şu an için maxlen eşittir 5'ten kurtulacağım. Sonra oluşturduğumuz word dizinini, oluşturduğumuz dizileri ve oluşturduğumuz dolgu dizilerini yazdıracağız. Şimdi araştıralım ve onlara bir göz atalım.
Cümleleri hatırlıyorsan, köpeklerimi seviyorum, kedimi seviyorum, köpeğimi seviyorsun, sence köpeğim harika mı? İşte onlardan oluşturulan kelime dizini. Kelime dağarcığı belirtecinin değeri 1, benimki 2, aşk 3, köpek 4, ben 5 vb. Şimdi bizim için yaratılan dizilere bakarsak. İlk cümleyi hatırlarsan, köpeğimi seviyorum. 5,3,2,4'e bakarsanız, ben 5'im, aşk 3'üm, benim 2'm, köpek 4'üm ikinci cümle, kedimi seviyorum. Ben 5, aşk 3, benim 2, kedi 7 vb. Bunlar bizim dört cümlemiz bu dizilere kodlanmış. Şimdi yastıklı dizileri yaptığımızda hepsini aynı boyutta elde edeceğiz. Burada görebiliyorsanız, ilk üç cümlemizin uzunluğu dörttür. İşte bu, bu ve bu. Doğru uzunluğu elde etmek için sonunda sıfır karakterden oluştuklarını göreceksiniz. Daha sonra, daha uzun olan son cümlemiz, matris bu cümle ile aynı genişliğe sahiptir. Yani eğer yukarıda, maxlen'in beşe eşit olduğunu söyleseydim ve çalıştırsaydım. Şimdi yastıklı dizilerimin değiştiğini göreceğiz, böylece dörtler beş yapmak için yalnızca bir karakterle doldurulacak. Son cümlem olan 8,6,9,2,4, bu son üç karakteri aynı genişlikte yapmak için kaybetti çünkü kesmem gönderiye eşittir. Kesmeyi kaldırırsam post'a eşittir ve aslında post'a eşit dolguyu da kaldırırım. Ne olacağını görebiliriz. Şimdi göreceğiz ki, dolgum ön olduğundan, bu üç kısa cümlenin önüne sıfır eklenir. Kesişim önceden olduğu için, bu filmdeki son beş karakteri aldığımızı göreceğiz. Dolguya gerek yok, ancak kesik ön tarafta. Tokenizer ve padding, metin ön işleminizde size yardımcı olmak için gerçekten böyle çalışır. Orada çok güzel şeyler oluyor. Şimdi ikinci şey, belirteçleştiricinin sabitlenmediği kelimelere bir göz atmaktı. Tokenizer'ı ne zaman yarattığımızı hatırlarsanız ve buradaki belirteçlerin listesi sadece bu cümlelerden alınmıştır. Örneğin, bunlardan farklı cümlelere bakmaya başladığımızda köpeğimi gerçekten seviyorum. Köpeğimi seveceğime, bunun için kelime dağarcığından çıkmış bir karakter alacağımızı fark edeceğiz, çünkü bu öğrenilmedi. Bu verilerle sinir ağlarını eğitirken bu daha sonra oldukça önemli hale gelir. Çünkü onu sadece buradaki belirteçler üzerinde eğitecekseniz, bu gibi cümleleri sınıflandırmak zorlaşacaktır çünkü belirteçleştiricinin daha önce görmediği kelimeler vardır ve bu nedenle sinir ağı üzerinde eğitilmeyecektir. Yine, sahip olduğunuz daha fazla eğitim verisinin o kadar iyi olduğu klasik bir durumdur. Ancak yine de tokenizer'ın bu senaryoda nasıl davranacağına bir göz atalım. Bu durumda, tokenizer'ı tekrar kullandığım test verilerinden bir test dizisi oluşturacağım ve bazı diziler oluşturmak için test verilerini aktarıyorum. Bunları yazdıracağım ve sonra bunları not defterine yazdıracağım. Yine, onları doldurduğumu görebilirsiniz, burada herhangi bir davranış belirtmiyorum. Maksimum uzunluk, en uzun cümlenin uzunluğu olacaktır. Dolgu önceden olacak ve kesme önceden olacaktır. Eğer onu çalıştırırsam ve onlara bir göz atarsak. Şimdi test dizilerimin oldukça ilginçleştiğini görebiliyoruz çünkü köpeğimi gerçekten sevdiğimi söyledim. Benim test sırası 5, 1, 3, 2, 4. Beşi ben, biri elbette kelime dağarcığının dışında ve sonra 3,2,4 köpeğimi seviyor. Boş, köpeğimi seviyorum, test dizisinin etkili bir şekilde neye benzediğidir. Bu cümle, köpeğim denizayımı seviyor. Aşk kelimesi aşk kelimesinden farklıdır, bu yüzden onun için bir belirteç yoktur ve elbette denizayısı için bir belirteç yoktur. Ama köpeğimi ve benimkini daha önce gördük. Böylece 2,4,1,2,1 ile sonuçlanırız. Elbette ikisi benim, dördü köpek, biri kelime dağarcığım dışında, ikisi benim ve biri kelime dağarcığım dışında. Köpeğimi boşluğumu boşaltıyorum. Sonra bu kodu kullanarak dizileri doldurduğumda, 5,3,1,4 aynı kalacaktır. 2,4,1,2,1 aynı kalacaktır çünkü cümleler aynı uzunluktadır, bu nedenle dolgu yoktur ve kesilmeleri gerekir. Tabii burada maxlen'in 10'a eşit olduğu gibi bir şey söyleseydim ve sonra onu yönetirdim. Çok fazla dolgu alacağız ve varsayılan dolgu deneyimi önceden olduğundan, önceden doldurulacaktır. Bu, tokenizer'ın nasıl kullanılacağına ve metin ön işleme yapmak için ped dizilerinin nasıl kullanılacağına dair temel bir bakış. Bir sonraki videoda, sadece bu basit kodlanmış olanlar yerine çok daha büyük veri kümeleri yüklemeye ve onlar için ne yapacağımıza bir göz atacağız


## Sarcasm, really?
Bu hafta şimdiye kadar metinlere ve metnin nasıl tokenize edileceğine ve ardından Tensorflow'da bulunan araçları kullanarak cümleleri dizilere nasıl dönüştüreceğinize bakıyordunuz. Bunu çok basit kodlanmış cümleler kullanarak yaptınız. Ama elbette, gerçek dünya problemleri söz konusu olduğunda, bu basit cümlelerden çok daha fazla veri kullanıyor olacaksınız. Bu derste, bazı genel veri kümelerine ve bunları bir sinir ağını eğitmeye hazır hale getirmek için nasıl işleyebileceğinize bir göz atacağız. Rishabh Misra tarafından yayınlanan Kaggle ile ilgili ayrıntıları bu bağlantıda yayınlayarak başlayacağız. Alay tespitinin her yerinde gerçekten eğlenceli bir CC0 kamu malı veri seti. Gerçekten? Evet, gerçekten. Bu veri seti çok basit ve basittir, çalışması çok kolay olduğundan bahsetmiyorum bile. İçinde üç element var. Birincisi alaycı, etiketimiz. Kayıt alaycı kabul edilirse birdir, aksi takdirde sıfırdır. İkincisi, yalnızca düz metin olan bir başlık ve üçüncüsü, başlığın açıkladığı makaleye bağlantıdır. html'nin içeriğini ayrıştırmak, komut dosyalarını, stilleri vb. Çıkarmak, bu dersin kapsamının biraz ötesindedir. Bu yüzden sadece manşetlere odaklanacağız. Verileri o Kaggle sitesinden indirirseniz, bunun gibi bir şey görürsünüz. Gördüğünüz gibi, adın makale bağlantısı, başlık ve is_sarcastic olduğu ve değerlerin gösterildiği gibi olduğu ad-değer çiftlerine sahip bir liste girdileri kümesidir. Bu verilerin Python'a yüklenmesini çok daha kolay hale getirmek için, verilere bu şekilde bakmak için biraz değişiklik yaptım, bunu yapmaktan çekinmeyin veya değiştirilmiş veri setimi ortak laboratuvardaki bağlantıdan indirebilirsiniz. dersin bu kısmı için. Böyle bir veriye sahip olduğunuzda, onu Python'a yüklemek gerçekten kolaydır. Koda bir göz atalım. Yani önce json'u içe aktarmanız gerekiyor. Bu, verileri JSON biçiminde yüklemenize ve ondan otomatik olarak bir Python veri yapısı oluşturmanıza olanak tanır. Bunu yapmak için dosyayı açmanız ve json'a aktarmanız yeterlidir.yükleyin ve üç tür verinin listesini içeren bir liste alırsınız: başlıklar, URL'ler ve is_sarcastic etiketleri. Cümlelerin kendi listelerinin bir listesi olarak belirteçleştiriciye geçmesini istediğim için, daha sonra bir cümle listesi oluşturabilirim ve daha sonra, bir sinir ağı oluşturmak için etiketleri istersem, bunların bir listesini de oluşturabilirim. Bunu yaparken, burada kullanmayacağım halde URL'leri de yapabilirim, ancak isteyebilirsiniz. Artık veri deposu döngüsünde bir for öğesi ile oluşturulan listede yineleme yapabilirim. Her öğe için başlığı cümlelerime, is_sarcastic etiketlerime ve article_link url'lerime kopyalayabilirim. Şimdi tokenizer'da çalışabileceğim bir şeyim var, o yüzden bir sonrakine bakalım.